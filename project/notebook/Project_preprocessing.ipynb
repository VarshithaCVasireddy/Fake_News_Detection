{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## This code is used to preprocess the data for the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pLot2zFo4kum"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Import STOPWORDS from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import string, re\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtUKukXi5Lve",
        "outputId": "b1b0665d-0565-4dfd-cc4f-088bdb2298d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.14.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.38)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.6)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1 jedi-0.18.2\n",
            "time: 235 µs (started: 2023-04-30 05:40:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahN6mmD65N84",
        "outputId": "8d459b05-4722-4375-e4df-c594604280b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 784 ms (started: 2023-04-30 05:40:48 +00:00)\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/VarshithaCVasireddy/datasets/main/dataset_new_feature.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XyCFT935jY8",
        "outputId": "79b3edcb-24c2-4e06-9dd5-04ed93442807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "time: 18.8 s (started: 2023-04-30 05:40:52 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYblYG3-5Xtm",
        "outputId": "4074fcec-0588-44e4-ef32-449c0c5b4659"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 2.99 s (started: 2023-04-28 21:28:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "\n",
        "# Load the RoBERTa tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT4Vp_dXpahB",
        "outputId": "f9c05f22-4f7e-4347-81ed-ac64ef510ef1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 375 ms (started: 2023-04-28 21:28:50 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Set the device to use for computation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ZB7E2y5Y9P",
        "outputId": "430a173e-75eb-4b31-f942-ba8d71dc7cb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 7min 34s (started: 2023-04-28 21:28:58 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Define a list to store the predicted sentiment labels\n",
        "predicted_sentiments = []\n",
        "\n",
        "# Iterate over the titles in your dataset and classify them\n",
        "for title in data['str1']:\n",
        "    # Tokenize the title and convert it to input features for the model\n",
        "    inputs = tokenizer(title, return_tensors='pt', truncation=True, padding=True)\n",
        "    inputs.to('cuda')\n",
        "\n",
        "    # Use the model to predict the sentiment label for the title\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_sentiment = torch.argmax(logits).item()\n",
        "\n",
        "    # Map the predicted sentiment index to a string label\n",
        "    predicted_sentiment_label = 'positive' if predicted_sentiment == 1 else 'negative'\n",
        "\n",
        "    # Append the predicted sentiment label to the list\n",
        "    predicted_sentiments.append(predicted_sentiment_label)\n",
        "\n",
        "# Add a new column to the dataset containing the predicted sentiment labels\n",
        "data['predicted_sentiment'] = predicted_sentiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAoVMmvFrMnM",
        "outputId": "efd301aa-9bbf-40d9-fb37-977020eaf195"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 9min 48s (started: 2023-04-30 05:43:18 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "\n",
        "# Load the GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
        "\n",
        "model.eval()\n",
        "# Set the device to use for computation\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Move the model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Define a list to store the predicted sentiment labels\n",
        "predicted_sentiments_gpt2 = []\n",
        "\n",
        "# Iterate over the titles in your dataset and classify them\n",
        "for title in data['str1']:\n",
        "    # Tokenize the title and convert it to input features for the model\n",
        "    inputs = tokenizer(title, return_tensors='pt', truncation=True)\n",
        "    inputs.to(device)\n",
        "\n",
        "    # Use the model to predict the sentiment label for the title\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_sentiment = torch.argmax(logits).item()\n",
        "\n",
        "    # Map the predicted sentiment index to a string label\n",
        "    predicted_sentiment_label = 'positive' if predicted_sentiment == 1 else 'negative'\n",
        "\n",
        "    # Append the predicted sentiment label to the list\n",
        "    predicted_sentiments.append(predicted_sentiment_label)\n",
        "\n",
        "# Add a new column to the dataset containing the predicted sentiment labels\n",
        "data['predicted_sentiment_gpt2'] = predicted_sentiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUUm9jW-6ClM",
        "outputId": "012cffbf-f668-403c-ea69-3ca9280a7508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 446 µs (started: 2023-04-30 05:55:11 +00:00)\n"
          ]
        }
      ],
      "source": [
        "csv_path = '/content/drive/MyDrive/sentiment_analysis_gpt2.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JwSFzU-7T__",
        "outputId": "8a85ccf2-6995-4cfa-a87d-1f540a40fa7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 267 ms (started: 2023-04-30 05:56:09 +00:00)\n"
          ]
        }
      ],
      "source": [
        "data.to_csv(csv_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3v4I5E9_8Mw2",
        "outputId": "cb15ea4c-6dc8-42bd-9e35-52f562082858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 551 µs (started: 2023-04-30 05:57:33 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Define the labels to be used for classification\n",
        "labels = [\"politics\", \"sports\", \"entertainment\", \"health\", \"technology\", \"education\", \"environment\", \"lifestyle\", \"crime\", \"religion\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVP6Vfor-EhL",
        "outputId": "e648fd76-6284-48bc-bfb0-a29066d8d5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 9min 44s (started: 2023-04-30 05:59:33 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Define a list to store the predicted labels\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate over the titles in your dataset and classify them\n",
        "for title in data['str1']:\n",
        "    # Tokenize the title and convert it to input features for the model\n",
        "    inputs = tokenizer(title, return_tensors='pt', truncation=True)\n",
        "    inputs.to(device)\n",
        "    \n",
        "    # Use the model to predict the label for the title\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_label = labels[torch.argmax(logits).item()]\n",
        "    \n",
        "    # Append the predicted label to the list\n",
        "    predicted_labels.append(predicted_label)\n",
        "\n",
        "# Add a new column to the dataset containing the predicted labels\n",
        "data['predicted_label_gpt2'] = predicted_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO2iu4E_ruY4"
      },
      "source": [
        "## For checking click-bait words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2TAJr3DuVc4",
        "outputId": "95cec45a-1311-4582-b11b-b9b05bd0c41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 334 ms (started: 2023-04-28 21:16:41 +00:00)\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/VarshithaCVasireddy/datasets/main/dataset_cleaned.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHqf-Mg5A71b",
        "outputId": "b8650704-8c4c-42a7-8cca-84c31009f8e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 647 µs (started: 2023-04-30 06:19:49 +00:00)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Import STOPWORDS from NLTK\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import string, re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-3ZAdM2B-IF",
        "outputId": "c391fe09-f9ea-4d8a-f102-0ffa904ede5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1.25 s (started: 2023-04-28 21:16:46 +00:00)\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB0UshlFB3gA",
        "outputId": "f5f2d47e-7f13-4464-a400-d96be5ce74fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 755 ms (started: 2023-04-28 21:16:50 +00:00)\n"
          ]
        }
      ],
      "source": [
        "data.title = [Text.lower() for Text in data.title]\n",
        "def sentence_tokenize(text: str):\n",
        "    return nltk.sent_tokenize(text)\n",
        "\n",
        "data['Sentences'] = data['title'].apply(sentence_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZDqAgJYCDFK",
        "outputId": "9f278d5a-4b47-4056-a90c-f27845cbb2c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 682 µs (started: 2023-04-28 21:16:55 +00:00)\n"
          ]
        }
      ],
      "source": [
        "clickbait_words = {\n",
        "    \"shocking\", \"believe\", \"amazing\", \"secrets\", \"revealed\",\n",
        "    \"mind-blowing\", \"unbelievable\", \"incredible\", \"insane\", \"must see\",\n",
        "    \"astonishing\", \"shocking truth\", \"exposed\", \"epic\", \"life-changing\",\n",
        "    \"amazing discovery\", \"little-known\", \"startling\", \"forbidden\",\n",
        "    \"once-in-a-lifetime\", \"jaw-dropping\", \"bizarre\", \"scandalous\", \"sensational\",\n",
        "    \"remarkable\", \"bombshell\", \"proof\", \"cover-up\", \"outrageous\",\n",
        "    \"breaking\", \"uncovered\", \"conspiracy\", \"lies\", \"hoax\",\n",
        "    \"oops\", \"omg\", \"watch\", \"suprise\", \"confirmed\",\n",
        "    \"terrified\", \"terrifying\", \"alert\", \"outrage\", \"wow\", \"bam\",\n",
        "    \"shaking\", \"boom\", \"outstanding\", \"awesome\", \"ouch\", \"brilliant\",\n",
        "    \"wikileaks\", \"lol\", \"stunning\", \"unreal\", \"epic\", \"must read\",\n",
        "    \"outrageous\", \"revealed\", \"explained\", \"exclusive\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSh1l1zdCIWr",
        "outputId": "02d2d6b5-acb2-4ec1-9fe9-538672d701d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 58.1 s (started: 2023-04-28 21:16:57 +00:00)\n"
          ]
        }
      ],
      "source": [
        "def word_tokenize(sentences: list[str]):\n",
        "    words: list[str] = []\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for sent in sentences:\n",
        "        for word in nltk.word_tokenize(sent):\n",
        "            if word not in stopwords.words('english') and word not in string.punctuation:\n",
        "                word = re.sub(r\"[^a-zA-Z0-9\\s.-]\", \"\", word)\n",
        "                words.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    return words \n",
        "\n",
        "data['Words'] = data['Sentences'].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNscHuG2CL9z",
        "outputId": "5a3ccf9b-3d80-48da-ffdb-0bcf90b8e37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 4.33 s (started: 2023-04-28 21:18:08 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# create two new columns to store matched words and count\n",
        "data['matched_words'] = ''\n",
        "data['matched_count'] = 0\n",
        "\n",
        "# loop through each row in the dataframe\n",
        "for index, row in data.iterrows():\n",
        "    # loop through each sentence in the list of sentences for the row\n",
        "    for sentence in row['Sentences']:\n",
        "        # use regex to find all the words in the sentence that match the clickbait words list\n",
        "        matched = re.findall(r'\\b(' + '|'.join(clickbait_words) + r')\\b', sentence)\n",
        "        # update the matched words and count columns for the row\n",
        "        if len(matched) > 1:\n",
        "            data.at[index, 'matched_words'] += ', '.join(matched[:-1]) + ', ' + matched[-1]\n",
        "        elif len(matched) == 1:\n",
        "            data.at[index, 'matched_words'] += matched[0]\n",
        "        data.at[index, 'matched_count'] += len(matched)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfM6Zi3ECR3b",
        "outputId": "afb6e53a-fd9c-443c-cebf-39a948369147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 366 ms (started: 2023-04-28 21:18:25 +00:00)\n"
          ]
        }
      ],
      "source": [
        "data['str1'] = [' '.join(doc) for doc in data.Words]\n",
        "# Create a new column 'Diff_Charac' which counts the characters in the sentence title which are not alpha numerics and space\n",
        "data['Diff_Charac'] = data['title'].apply(lambda x: sum(1 for char in x if not char.isalnum() and not char.isspace()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
